{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.stats import chi2_contingency\n",
    "import configparser\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_config():\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(\"../config.ini\")\n",
    "    return config\n",
    "\n",
    "def download_blob(config):\n",
    "    \"\"\"Downloads a blob from the bucket.\"\"\"\n",
    "    \n",
    "    # Initialize a storage client\n",
    "    path = config['gcp']['raw_data']\n",
    "    df = pd.read_csv(path)\n",
    "    return df\n",
    "\n",
    "config = init_config()\n",
    "df = download_blob(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "for column in categorical_columns:\n",
    "    df[column] = df[column].str.replace(' ', '_').str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Department = df.Department.map(lambda x : \"research_and_development\" if x == 'research_&_development' else x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns = df.select_dtypes(include=['int64'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_replace_bins(df, columns, num_bins=5):\n",
    "    binned_df = df.copy()\n",
    "    for col in columns:\n",
    "        col_range = df[col].max() - df[col].min()\n",
    "        if col_range > 5:\n",
    "            # Calculate quantile-based bins\n",
    "            _, bins = pd.qcut(df[col], q=num_bins, duplicates='drop', retbins=True)\n",
    "            # Adjust bins to integer values\n",
    "            bins = np.floor(bins).astype(int)\n",
    "            bins[-1] = bins[-1] + 1  # Ensure the last bin is inclusive\n",
    "            # Create labels based on bin edges\n",
    "            labels = [f'{bins[i]}<=x<{bins[i+1]}' for i in range(len(bins)-1)]\n",
    "            # Apply binning and replace original column\n",
    "            binned_df[col] = pd.cut(df[col], bins=bins, labels=labels, right=False, include_lowest=True)\n",
    "    return binned_df\n",
    "\n",
    "# Apply dynamic binning and replace columns with integer bins where range is > 5\n",
    "df = create_and_replace_bins(df, numerical_columns, num_bins=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_binned_value_counts(df, columns):\n",
    "    value_counts_dict = {}\n",
    "    for col in columns:\n",
    "        value_counts_dict[col] = df[col].value_counts().to_dict()\n",
    "    return value_counts_dict\n",
    "\n",
    "# Get value counts for each binned column\n",
    "get_binned_value_counts(df, numerical_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Attrition'] = df['Attrition'].map(lambda x : 0 if x == 'no' else 1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select_dtypes(include=['object'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding categorical variables\n",
    "df['Gender'] = df['Gender'].map({'male': 1, 'female': 0})\n",
    "df['MaritalStatus'] = df['MaritalStatus'].map({'single': 0, 'married': 1, 'divorced': 2})\n",
    "\n",
    "# Contingency tables\n",
    "contingency_table_gender = pd.crosstab(df['Gender'], df['Attrition'])\n",
    "contingency_table_marital = pd.crosstab(df['MaritalStatus'], df['Attrition'])\n",
    "\n",
    "print(\"Contingency Table - Gender vs Target:\")\n",
    "print(contingency_table_gender)\n",
    "\n",
    "print(\"\\nContingency Table - Marital Status vs Target:\")\n",
    "print(contingency_table_marital)\n",
    "\n",
    "# Chi-Square tests\n",
    "chi2_gender, p_gender, dof_gender, ex_gender = chi2_contingency(contingency_table_gender)\n",
    "chi2_marital, p_marital, dof_marital, ex_marital = chi2_contingency(contingency_table_marital)\n",
    "\n",
    "print(f\"\\nChi-Square Test between gender and target: chi2 = {chi2_gender}, p-value = {p_gender}\")\n",
    "print(f\"Chi-Square Test between marital_status and target: chi2 = {chi2_marital}, p-value = {p_marital}\")\n",
    "\n",
    "# Interpretation\n",
    "alpha = 0.05\n",
    "print(\"\\nInterpretation:\")\n",
    "if p_gender < alpha:\n",
    "    print(\"There is a significant association between gender and target (p < 0.05).\")\n",
    "else:\n",
    "    print(\"There is no significant association between gender and target (p >= 0.05).\")\n",
    "\n",
    "if p_marital < alpha:\n",
    "    print(\"There is a significant association between marital status and target (p < 0.05).\")\n",
    "else:\n",
    "    print(\"There is no significant association between marital status and target (p >= 0.05).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['Gender','StandardHours', 'Over18', 'EmployeeCount', 'EmployeeNumber', 'BusinessTravel'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Business Travel\n",
    "\n",
    "# df['BusinessTravel'].value_counts()\n",
    "# # in this case we will use label encoding -- you should check the reason !! we could have choosen one-hot but we didn't ...\n",
    "# df['BusinessTravel'] = df['BusinessTravel'].apply(lambda x: 0 if x =='Non-Travel' else (1 if x == 'Travel_Rarely' else 2))\n",
    "\n",
    "# Department\n",
    "\n",
    "df['Department'].value_counts()\n",
    "department_dummies = pd.get_dummies(df['Department'], prefix='Department', dtype=float) # why now we used one-hot encoding and not label encoding?\n",
    "df.drop(columns=['Department'], inplace=True)\n",
    "df = pd.concat([df,department_dummies], axis=1)\n",
    "\n",
    "# Education Field and JobRole\n",
    "\n",
    "for field in ['JobRole', 'EducationField']:\n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "    new_data = lb.fit_transform(df[field])\n",
    "    binary_df = pd.DataFrame(new_data, columns=[f\"{field}_{cls}\" for cls in lb.classes_])\n",
    "    df = pd.concat([df.drop(columns=[field]), binary_df], axis=1)\n",
    "\n",
    "\n",
    "# OverTime\n",
    "\n",
    "df['OverTime'] = df['OverTime'].map({'yes': 1, 'no': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTENC\n",
    "\n",
    "desired_minority_count = int(0.3 * 1233)\n",
    "sampling_strategy = {0: 1233, 1: desired_minority_count}\n",
    "X_train = df.drop(columns=['Attrition'])\n",
    "y_train = df['Attrition']\n",
    "\n",
    "smotenc = SMOTENC(categorical_features=\"auto\", sampling_strategy=sampling_strategy, random_state=42)\n",
    "X_train_resampled, y_train_resampled = smotenc.fit_resample(X_train, y_train)\n",
    "\n",
    "# Combine resampled features with target\n",
    "df = pd.concat([pd.DataFrame(X_train_resampled, columns=X_train.columns), pd.DataFrame(y_train_resampled, columns=['Attrition'])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Attrition.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"gs://pa-poc-mlspec-3-cs/pre-processed4.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_config():\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(\"../config.ini\")\n",
    "    return config\n",
    "\n",
    "config = init_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create a dataset \n",
    "\n",
    "import os\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "PROJECT_ID = config['gcp']['project']\n",
    "REGION = config['gcp']['region']\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
    "\n",
    "dataset = aiplatform.TabularDataset.create(\n",
    "    display_name=\"HR Analytics3\",\n",
    "    gcs_source=\"gs://pa-poc-mlspec-3-cs/pre-processed4.csv\",\n",
    ")\n",
    "\n",
    "label_column = \"Attrition\"\n",
    "\n",
    "print(dataset.resource_name)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = aiplatform.AutoMLTabularTrainingJob(\n",
    "  display_name=\"train-automl-hr-analytics1\",\n",
    "  optimization_prediction_type=\"classification\",\n",
    "  optimization_objective=\"maximize-au-prc\",\n",
    ")\n",
    "\n",
    "model = job.run(\n",
    "    dataset=dataset,\n",
    "    target_column=label_column,\n",
    "    training_fraction_split=0.6,\n",
    "    validation_fraction_split=0.2,\n",
    "    test_fraction_split=0.2,\n",
    "    budget_milli_node_hours=1000,\n",
    "    model_display_name=\"test1\",\n",
    "    disable_early_stopping=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = model.deploy(deployed_model_display_name='test1',\n",
    "    machine_type='n1-standard-4'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import subprocess\n",
    "from pprint import pprint\n",
    "\n",
    "# Get the access token using gcloud\n",
    "access_token = (\n",
    "    subprocess.check_output(\"gcloud auth print-access-token\", shell=True)\n",
    "    .decode(\"utf-8\")\n",
    "    .strip()\n",
    ")\n",
    "\n",
    "\n",
    "# Define the project ID, endpoint ID, and input data file\n",
    "project_id = \"121050757542\"\n",
    "endpoint_id = \"1928034321235443712\"\n",
    "\n",
    "input_data = {\"instances\": [[1.0, 2.0, 17.0, 2.0, 0.0, 1.0, 5.0, 8.0]]}\n",
    "# Define the endpoint URL\n",
    "url = f\"https://us-central1-aiplatform.googleapis.com/v1/projects/{project_id}/locations/us-central1/endpoints/{endpoint_id}:predict\"\n",
    "\n",
    "# Define the headers\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {access_token}\",\n",
    "    \"Content-Type\": \"application/json\",\n",
    "}\n",
    "\n",
    "# Send the POST request\n",
    "response = requests.post(url, headers=headers, json=input_data)\n",
    "\n",
    "# Print the response\n",
    "pprint(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEST WITH CALLABLE API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import subprocess\n",
    "from pprint import pprint\n",
    "\n",
    "# Get the access token using gcloud\n",
    "access_token = (\n",
    "    subprocess.check_output(\"gcloud auth print-access-token\", shell=True)\n",
    "    .decode(\"utf-8\")\n",
    "    .strip()\n",
    ")\n",
    "\n",
    "\n",
    "# Define the project ID, endpoint ID, and input data file\n",
    "project_id = config['prediction']['project_id']\n",
    "endpoint_id = config['prediction']['endpoint_id']\n",
    "\n",
    "input_data = {\n",
    "    \"instances\": [\n",
    "        {\n",
    "            \"Age\": \"29 <= x <= 34\",\n",
    "            \"DailyRate\": \"102 <= x <= 391\",\n",
    "            \"DistanceFromHome\": \"2 <= x <= 5\",\n",
    "            \"Education\": \"3\",\n",
    "            \"EnvironmentSatisfaction\": \"3\",\n",
    "            \"HourlyRate\": \"45 <= x <= 59\",\n",
    "            \"JobInvolvement\": \"3\",\n",
    "            \"JobLevel\": \"1\",\n",
    "            \"JobSatisfaction\": \"3\",\n",
    "            \"MaritalStatus\": \"1\",\n",
    "            \"MonthlyIncome\": \"1009 <= x <= 2695\",\n",
    "            \"MonthlyRate\": \"6887 <= x <= 11773\",\n",
    "            \"NumCompaniesWorked\": \"1 <= x <= 3\",\n",
    "            \"OverTime\": \"0\",\n",
    "            \"PercentSalaryHike\": \"13 <= x <= 15\",\n",
    "            \"PerformanceRating\": \"3\",\n",
    "            \"RelationshipSatisfaction\": \"3\",\n",
    "            \"StockOptionLevel\": \"0\",\n",
    "            \"TotalWorkingYears\": \"10 <= x <= 17\",\n",
    "            \"TrainingTimesLastYear\": \"2 <= x <= 3\",\n",
    "            \"WorkLifeBalance\": \"3\",\n",
    "            \"YearsAtCompany\": \"2 <= x <= 5\",\n",
    "            \"YearsInCurrentRole\": \"2 <= x <= 4\",\n",
    "            \"YearsSinceLastPromotion\": \"0 <= x <= 1\",\n",
    "            \"YearsWithCurrManager\": \"2 <= x <= 4\",\n",
    "            \"Department_human_resources\": \"0.0\",\n",
    "            \"Department_research_and_development\": \"1.0\",\n",
    "            \"Department_sales\": \"0.0\",\n",
    "            \"JobRole_healthcare_representative\": \"0\",\n",
    "            \"JobRole_human_resources\": \"0\",\n",
    "            \"JobRole_laboratory_technician\": \"0\",\n",
    "            \"JobRole_manager\": \"0\",\n",
    "            \"JobRole_manufacturing_director\": \"0\",\n",
    "            \"JobRole_research_director\": \"0\",\n",
    "            \"JobRole_research_scientist\": \"0\",\n",
    "            \"JobRole_sales_executive\": \"0\",\n",
    "            \"JobRole_sales_representative\": \"0\",\n",
    "            \"EducationField_human_resources\": \"0\",\n",
    "            \"EducationField_life_sciences\": \"0\",\n",
    "            \"EducationField_marketing\": \"0\",\n",
    "            \"EducationField_medical\": \"0\",\n",
    "            \"EducationField_other\": \"0\",\n",
    "            \"EducationField_technical_degree\": \"0\",\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "url = f\"https://europe-west2-aiplatform.googleapis.com/v1/projects/{project_id}/locations/europe-west2/endpoints/{endpoint_id}:predict\"\n",
    "\n",
    "# Define the headers\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {access_token}\",\n",
    "    \"Content-Type\": \"application/json\",\n",
    "}\n",
    "\n",
    "# Send the POST request\n",
    "response = requests.post(url, headers=headers, json=input_data)\n",
    "\n",
    "# Print the response\n",
    "pprint(response.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "# Initialize the Google Cloud Storage client\n",
    "storage_client = storage.Client()\n",
    "\n",
    "# Function to resize an image\n",
    "def resize_image(image_bytes, target_size=(224, 224)):\n",
    "    img = Image.open(io.BytesIO(image_bytes))\n",
    "    img_resized = img.resize(target_size)\n",
    "    output_bytes = io.BytesIO()\n",
    "    img_resized.save(output_bytes, format=img.format)\n",
    "    return output_bytes.getvalue()\n",
    "\n",
    "# Function to process images from GCS\n",
    "def process_images_from_gcs(bucket_name, source_folder, target_folder, target_size=(224, 224)):\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "    # List all images in the source folder\n",
    "    blobs = bucket.list_blobs(prefix=source_folder)\n",
    "\n",
    "    for blob in blobs:\n",
    "        if blob.name.endswith(\".jpg\"):  # Check for image files (adjust for your format)\n",
    "            # Download the image as bytes\n",
    "            image_bytes = blob.download_as_bytes()\n",
    "\n",
    "            # Resize the image\n",
    "            resized_image_bytes = resize_image(image_bytes, target_size)\n",
    "\n",
    "            # Define the target path in GCS\n",
    "            target_path = f\"{target_folder}/{blob.name.split('/')[-1]}\"\n",
    "\n",
    "            # Upload the resized image back to GCS\n",
    "            new_blob = bucket.blob(target_path)\n",
    "            new_blob.upload_from_string(resized_image_bytes, content_type=\"image/jpeg\")\n",
    "\n",
    "            print(f\"Resized and uploaded: {target_path}\")\n",
    "\n",
    "# Example usage\n",
    "bucket_name = \"pa-poc-mlspec-3-cs\"\n",
    "source_folder = \"path/to/source/images\"\n",
    "target_folder = \"path/to/resized/images\"\n",
    "process_images_from_gcs(bucket_name, source_folder, target_folder)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
