{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import configparser\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\"../config.ini\")\n",
    "\n",
    "# Read both CSV files\n",
    "fashion_dataset_images = pd.read_csv('../data/images.csv', on_bad_lines='skip')\n",
    "fashion_dataset_styles = pd.read_csv('../data/styles.csv', on_bad_lines='skip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fashion Dataset Merged Shape: (44424, 12)\n",
      "\n",
      "Fashion Dataset Merged Preview:\n",
      "    filename                                               link     id gender  \\\n",
      "0  15970.jpg  http://assets.myntassets.com/v1/images/style/p...  15970    Men   \n",
      "1  39386.jpg  http://assets.myntassets.com/v1/images/style/p...  39386    Men   \n",
      "2  59263.jpg  http://assets.myntassets.com/v1/images/style/p...  59263  Women   \n",
      "3  21379.jpg  http://assets.myntassets.com/v1/images/style/p...  21379    Men   \n",
      "4  53759.jpg  http://assets.myntassets.com/v1/images/style/p...  53759    Men   \n",
      "\n",
      "  masterCategory subCategory  articleType baseColour  season    year   usage  \\\n",
      "0        Apparel     Topwear       Shirts  Navy Blue    Fall  2011.0  Casual   \n",
      "1        Apparel  Bottomwear        Jeans       Blue  Summer  2012.0  Casual   \n",
      "2    Accessories     Watches      Watches     Silver  Winter  2016.0  Casual   \n",
      "3        Apparel  Bottomwear  Track Pants      Black    Fall  2011.0  Casual   \n",
      "4        Apparel     Topwear      Tshirts       Grey  Summer  2012.0  Casual   \n",
      "\n",
      "                              productDisplayName  \n",
      "0               Turtle Check Men Navy Blue Shirt  \n",
      "1             Peter England Men Party Blue Jeans  \n",
      "2                       Titan Women Silver Watch  \n",
      "3  Manchester United Men Solid Black Track Pants  \n",
      "4                          Puma Men Grey T-shirt  \n"
     ]
    }
   ],
   "source": [
    "fashion_dataset_images['id'] = fashion_dataset_images['filename'].str.replace('.jpg', '').astype(int)\n",
    "\n",
    "# Merge the datasets\n",
    "fashion_dataset_merged = pd.merge(fashion_dataset_images, fashion_dataset_styles, on='id', how='inner')\n",
    "\n",
    "# Display the results\n",
    "print(\"Fashion Dataset Merged Shape:\", fashion_dataset_merged.shape)\n",
    "\n",
    "# Optional: Display first few rows to verify the merge\n",
    "print(\"\\nFashion Dataset Merged Preview:\")\n",
    "print(fashion_dataset_merged.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_dataset_merged.drop(columns=['link','gender', 'masterCategory', 'articleType', 'baseColour', 'season', 'year', 'usage', 'productDisplayName'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>id</th>\n",
       "      <th>subCategory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15970.jpg</td>\n",
       "      <td>15970</td>\n",
       "      <td>Topwear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39386.jpg</td>\n",
       "      <td>39386</td>\n",
       "      <td>Bottomwear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59263.jpg</td>\n",
       "      <td>59263</td>\n",
       "      <td>Watches</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21379.jpg</td>\n",
       "      <td>21379</td>\n",
       "      <td>Bottomwear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>53759.jpg</td>\n",
       "      <td>53759</td>\n",
       "      <td>Topwear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44419</th>\n",
       "      <td>17036.jpg</td>\n",
       "      <td>17036</td>\n",
       "      <td>Shoes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44420</th>\n",
       "      <td>6461.jpg</td>\n",
       "      <td>6461</td>\n",
       "      <td>Flip Flops</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44421</th>\n",
       "      <td>18842.jpg</td>\n",
       "      <td>18842</td>\n",
       "      <td>Topwear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44422</th>\n",
       "      <td>46694.jpg</td>\n",
       "      <td>46694</td>\n",
       "      <td>Fragrance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44423</th>\n",
       "      <td>51623.jpg</td>\n",
       "      <td>51623</td>\n",
       "      <td>Watches</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44424 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        filename     id subCategory\n",
       "0      15970.jpg  15970     Topwear\n",
       "1      39386.jpg  39386  Bottomwear\n",
       "2      59263.jpg  59263     Watches\n",
       "3      21379.jpg  21379  Bottomwear\n",
       "4      53759.jpg  53759     Topwear\n",
       "...          ...    ...         ...\n",
       "44419  17036.jpg  17036       Shoes\n",
       "44420   6461.jpg   6461  Flip Flops\n",
       "44421  18842.jpg  18842     Topwear\n",
       "44422  46694.jpg  46694   Fragrance\n",
       "44423  51623.jpg  51623     Watches\n",
       "\n",
       "[44424 rows x 3 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fashion_dataset_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_dataset_merged.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Create new directory for matched images if it doesn't exist\n",
    "os.makedirs('../data/raw_images', exist_ok=True)\n",
    "\n",
    "# Get list of actual image files in images folder\n",
    "image_files = set(f for f in os.listdir('../data/fashion-dataset/images') if f.endswith('.jpg'))\n",
    "\n",
    "# Filter dataset to only include rows where filename exists in images folder\n",
    "fashion_dataset_filtered = fashion_dataset_merged[fashion_dataset_merged['filename'].isin(image_files)].copy()\n",
    "\n",
    "print(f\"Original dataset size: {len(fashion_dataset_merged)}\")\n",
    "print(f\"Filtered dataset size: {len(fashion_dataset_filtered)}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move matched images to new folder\n",
    "for filename in fashion_dataset_filtered['filename']:\n",
    "    src = os.path.join('../data/fashion-dataset/images', filename)\n",
    "    dst = os.path.join('../data/raw_images', filename)\n",
    "    if os.path.exists(src):\n",
    "        shutil.move(src, dst)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_dataset_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_dataset_merged.subCategory.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "clothing_items_top_10 = [\n",
    "    \"Topwear\",\n",
    "    \"Bottomwear\", \n",
    "    \"Innerwear\",\n",
    "    \"Dress\",\n",
    "    \"Loungewear and Nightwear\",\n",
    "    \"Saree\",\n",
    "    \"Headwear\", \n",
    "    \"Ties\",\n",
    "    \"Scarves\",\n",
    "    \"Apparel Set\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('../data/raw_matched_images', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_files = set(f for f in os.listdir('../data/raw_images') if f.endswith('.jpg'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_dataset_merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter dataset to only include rows where filename exists AND subcategory is in our list\n",
    "fashion_dataset_filtered = fashion_dataset_merged[\n",
    "    (fashion_dataset_merged['filename'].isin(image_files)) & \n",
    "    (fashion_dataset_merged['subCategory'].isin(clothing_items_top_10))\n",
    "].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>id</th>\n",
       "      <th>subCategory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15970.jpg</td>\n",
       "      <td>15970</td>\n",
       "      <td>Topwear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39386.jpg</td>\n",
       "      <td>39386</td>\n",
       "      <td>Bottomwear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21379.jpg</td>\n",
       "      <td>21379</td>\n",
       "      <td>Bottomwear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>53759.jpg</td>\n",
       "      <td>53759</td>\n",
       "      <td>Topwear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1855.jpg</td>\n",
       "      <td>1855</td>\n",
       "      <td>Topwear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44414</th>\n",
       "      <td>30614.jpg</td>\n",
       "      <td>30614</td>\n",
       "      <td>Topwear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44415</th>\n",
       "      <td>13496.jpg</td>\n",
       "      <td>13496</td>\n",
       "      <td>Topwear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44417</th>\n",
       "      <td>12544.jpg</td>\n",
       "      <td>12544</td>\n",
       "      <td>Topwear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44418</th>\n",
       "      <td>42234.jpg</td>\n",
       "      <td>42234</td>\n",
       "      <td>Topwear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44421</th>\n",
       "      <td>18842.jpg</td>\n",
       "      <td>18842</td>\n",
       "      <td>Topwear</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22049 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        filename     id subCategory\n",
       "0      15970.jpg  15970     Topwear\n",
       "1      39386.jpg  39386  Bottomwear\n",
       "3      21379.jpg  21379  Bottomwear\n",
       "4      53759.jpg  53759     Topwear\n",
       "5       1855.jpg   1855     Topwear\n",
       "...          ...    ...         ...\n",
       "44414  30614.jpg  30614     Topwear\n",
       "44415  13496.jpg  13496     Topwear\n",
       "44417  12544.jpg  12544     Topwear\n",
       "44418  42234.jpg  42234     Topwear\n",
       "44421  18842.jpg  18842     Topwear\n",
       "\n",
       "[22049 rows x 3 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fashion_dataset_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in fashion_dataset_filtered['filename']:\n",
    "    src = os.path.join('../data/raw_images', filename)\n",
    "    dst = os.path.join('../data/raw_matched_images', filename)\n",
    "    if os.path.exists(src):\n",
    "        shutil.move(src, dst)\n",
    "\n",
    "# Update the main dataframe\n",
    "fashion_dataset_merged = fashion_dataset_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_dataset_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subCategory\n",
       "Topwear                     15398\n",
       "Bottomwear                   2693\n",
       "Innerwear                    1808\n",
       "Dress                         478\n",
       "Loungewear and Nightwear      470\n",
       "Saree                         427\n",
       "Headwear                      293\n",
       "Ties                          258\n",
       "Scarves                       118\n",
       "Apparel Set                   106\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fashion_dataset_merged['subCategory'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the count of images per subcategory\n",
    "subcategory_counts = fashion_dataset_merged['subCategory'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_safe = fashion_dataset_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_dataset_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_safe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get current counts\n",
    "subcategory_counts = fashion_dataset_merged['subCategory'].value_counts()\n",
    "print(\"Before capping:\")\n",
    "print(subcategory_counts)\n",
    "\n",
    "# Initialize empty dataframe for results\n",
    "capped_dataset = pd.DataFrame()\n",
    "\n",
    "# For each subcategory, take up to 1000 images\n",
    "for subcategory in clothing_items_top_10:\n",
    "    subset = fashion_dataset_merged[fashion_dataset_merged['subCategory'] == subcategory]\n",
    "    if len(subset) > 1000:\n",
    "        subset = subset.sample(n=1000, random_state=42)  # random_state for reproducibility\n",
    "    capped_dataset = pd.concat([capped_dataset, subset])\n",
    "\n",
    "# Update the main dataframe\n",
    "fashion_dataset_merged = capped_dataset\n",
    "\n",
    "print(\"\\nAfter capping at 1000:\")\n",
    "print(fashion_dataset_merged['subCategory'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_dataset_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new directory for the capped dataset\n",
    "os.makedirs('capped_images', exist_ok=True)\n",
    "\n",
    "# Move images based on the capped dataset\n",
    "for filename in fashion_dataset_merged['filename']:\n",
    "    src = os.path.join('categorized_matched_images', filename)\n",
    "    dst = os.path.join('capped_images', filename)\n",
    "    if os.path.exists(src):\n",
    "        shutil.move(src, dst)\n",
    "\n",
    "# Verify the count of moved files\n",
    "moved_files = len([f for f in os.listdir('capped_images') if f.endswith('.jpg')])\n",
    "print(f\"Moved {moved_files} files to capped_images directory\")\n",
    "print(f\"Should match dataframe size: {len(fashion_dataset_merged)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "# Get current max ID to ensure new IDs don't overlap\n",
    "max_id = fashion_dataset_merged['id'].max()\n",
    "current_new_id = max_id + 1\n",
    "\n",
    "# Create augmentation layer\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomRotation(0.2),\n",
    "    tf.keras.layers.RandomZoom(0.2),\n",
    "    tf.keras.layers.RandomBrightness(0.2),\n",
    "    tf.keras.layers.RandomContrast(0.2),\n",
    "])\n",
    "\n",
    "# Function to augment single image\n",
    "def augment_image(image_path):\n",
    "    # Read and convert image to tensor\n",
    "    img = tf.keras.preprocessing.image.load_img(image_path)\n",
    "    img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "    img_array = tf.expand_dims(img_array, 0)\n",
    "    \n",
    "    # Augment\n",
    "    augmented_img = data_augmentation(img_array)\n",
    "    \n",
    "    # Convert back to PIL Image\n",
    "    augmented_img = tf.keras.preprocessing.image.array_to_img(augmented_img[0])\n",
    "    return augmented_img\n",
    "\n",
    "# Process each subcategory\n",
    "new_rows = []\n",
    "for category in clothing_items_top_10:\n",
    "    category_df = fashion_dataset_merged[fashion_dataset_merged['subCategory'] == category]\n",
    "    current_count = len(category_df)\n",
    "    \n",
    "    if current_count < 1000:\n",
    "        needed_augmentations = 1000 - current_count\n",
    "        print(f\"Augmenting {category}: need {needed_augmentations} more images\")\n",
    "        \n",
    "        # Randomly select images to augment (with replacement)\n",
    "        source_files = category_df['filename'].tolist()\n",
    "        \n",
    "        for _ in range(needed_augmentations):\n",
    "            # Select random source image\n",
    "            source_filename = random.choice(source_files)\n",
    "            source_path = os.path.join('capped_images', source_filename)\n",
    "            \n",
    "            # Generate new filename and ID\n",
    "            new_filename = f\"{current_new_id}.jpg\"\n",
    "            \n",
    "            # Augment and save image\n",
    "            augmented_img = augment_image(source_path)\n",
    "            augmented_img.save(os.path.join('capped_images', new_filename))\n",
    "            \n",
    "            # Add new row to dataframe\n",
    "            new_rows.append({\n",
    "                'filename': new_filename,\n",
    "                'id': current_new_id,\n",
    "                'subCategory': category\n",
    "            })\n",
    "            \n",
    "            current_new_id += 1\n",
    "\n",
    "# Add new rows to dataframe\n",
    "if new_rows:\n",
    "    new_df = pd.DataFrame(new_rows)\n",
    "    fashion_dataset_merged = pd.concat([fashion_dataset_merged, new_df], ignore_index=True)\n",
    "\n",
    "# Verify results\n",
    "print(\"\\nFinal counts per category:\")\n",
    "print(fashion_dataset_merged['subCategory'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('../data/fashion_dataset_merged.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'] = df['subCategory']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Create base directory for categorized images if it doesn't exist\n",
    "base_dir = '../data/categorized_images'\n",
    "if not os.path.exists(base_dir):\n",
    "    os.makedirs(base_dir)\n",
    "\n",
    "# Create subdirectories for each unique category\n",
    "categories = df['label'].unique()\n",
    "for category in categories:\n",
    "    category_dir = os.path.join(base_dir, category)\n",
    "    if not os.path.exists(category_dir):\n",
    "        os.makedirs(category_dir)\n",
    "\n",
    "# Move files from capped_images to their category folders\n",
    "for _, row in df.iterrows():\n",
    "    src_path = os.path.join('../data/capped_images', row['filename'])\n",
    "    dst_path = os.path.join(base_dir, row['label'], row['filename'])\n",
    "    \n",
    "    # Check if source file exists before attempting to move\n",
    "    if os.path.exists(src_path):\n",
    "        shutil.copy2(src_path, dst_path)  # Using copy2 to preserve metadata\n",
    "\n",
    "print(\"Images have been organized into their respective category folders\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\"../config.ini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column for GCS paths using same folder structure\n",
    "bucket_name = config['gcp']['bucket']   \n",
    "df['image_path'] = df.apply(\n",
    "    lambda row: f\"gs://{bucket_name}/categorised_images/{row['label']}/{row['filename']}\", \n",
    "    axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['filename', 'id', 'gcs_path'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('gs://{bucket}/fashion_dataset_processed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['subCategory'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the 'link' column to reflect the GCS path\n",
    "# bucket_name = \"your-gcs-bucket-name\"\n",
    "\n",
    "df['gcs_path'] \n",
    "# Create the GCS file path for each image\n",
    "df['gcs_path'] = df.apply(\n",
    "    lambda row: f\"gs://{bucket_name}/{row['subCategory']}/{row['filename']}\", axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = config['gcp']['project']\n",
    "region = config['gcp']['region']\n",
    "gcs_source = [f\"gs://{bucket_name}/clothing_dataset_processed.csv\"]\n",
    "from google.cloud import aiplatform\n",
    "aiplatform.init(project=project_id, location=region)\n",
    "dataset = aiplatform.ImageDataset.create(\n",
    "    display_name=\"multi_class_image_dataset\",\n",
    "    gcs_source=gcs_source,\n",
    "    import_schema_uri=aiplatform.schema.dataset.ioformat.image.single_label_classification,\n",
    ")\n",
    "\n",
    "print(f\"Dataset resource name: {dataset.resource_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['gcs_file_path'] = df['image_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['image_path'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder columns to put gcs_file_path before label\n",
    "df = df[['gcs_file_path', 'label']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the AutoML model\n",
    "model = aiplatform.AutoMLImageTrainingJob(\n",
    "    display_name=\"image_classification_training\",\n",
    "    prediction_type=\"classification\",  # Use \"classification\" for multi-class classification\n",
    "    multi_label=False,  # Set to True if it's a multi-label classification problem\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.run(\n",
    "    dataset=dataset,\n",
    "    model_display_name=\"image_classification_model\",\n",
    "    budget_milli_node_hours=8000,  # Budget in milli node hours (8,000 = 8 node hours)\n",
    "    disable_early_stopping=False,  # Early stopping for efficiency\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = model.deploy(\n",
    "    machine_type=\"n1-standard-4\",  # Adjust machine type as needed\n",
    "    min_replica_count=1,          # Minimum number of replicas\n",
    "    max_replica_count=1,          # Maximum number of replicas must match minimum for this model type\n",
    ")\n",
    "\n",
    "print(f\"Model deployed to endpoint: {endpoint.resource_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import json\n",
    "import requests\n",
    "from google.auth.transport.requests import Request\n",
    "from google.auth import default\n",
    "\n",
    "# Replace with your endpoint URL\n",
    "ENDPOINT_URL = \"\"\n",
    "\n",
    "# Authenticate and get an access token\n",
    "def get_access_token():\n",
    "    credentials, project = default()\n",
    "    credentials.refresh(Request())\n",
    "    return credentials.token\n",
    "\n",
    "# Prepare the payload\n",
    "def make_prediction(image_path):\n",
    "    # Load the image and encode it as base64\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        image_content = base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "    # Prepare the payload in the required format\n",
    "    payload = {\n",
    "        \"instances\": [\n",
    "            {\n",
    "                \"content\": image_content\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Get the access token\n",
    "    token = get_access_token()\n",
    "\n",
    "    # Set up headers\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {token}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    # Make the request\n",
    "    response = requests.post(ENDPOINT_URL, headers=headers, json=payload)\n",
    "\n",
    "    # Check response\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        raise Exception(f\"Prediction failed: {response.status_code}, {response.text}\")\n",
    "\n",
    "# Call the function with your image\n",
    "image_path = \"../data/23451234567.jpg\"  # Replace with the path to your image\n",
    "try:\n",
    "    prediction = make_prediction(image_path)\n",
    "    print(\"Prediction result:\")\n",
    "    print(json.dumps(prediction, indent=2))  # Pretty print the result\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
